{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6094530-0767-4f6c-9e57-c7b5e06e402d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: opencv-python in c:\\programdata\\anaconda3\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
      "  Cloning https://github.com/facebookresearch/detectron2.git to c:\\users\\alex\\appdata\\local\\temp\\pip-req-build-wrv6rz3f\n",
      "  Resolved https://github.com/facebookresearch/detectron2.git to commit fd27788985af0f4ca800bca563acdb700bb890e2\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: Pillow>=7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from detectron2==0.6) (11.1.0)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from detectron2==0.6) (3.10.0)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from detectron2==0.6) (2.0.11)\n",
      "Requirement already satisfied: termcolor>=1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from detectron2==0.6) (3.1.0)\n",
      "Requirement already satisfied: yacs>=0.1.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from detectron2==0.6) (0.1.8)\n",
      "Requirement already satisfied: tabulate in c:\\programdata\\anaconda3\\lib\\site-packages (from detectron2==0.6) (0.9.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\programdata\\anaconda3\\lib\\site-packages (from detectron2==0.6) (3.0.0)\n",
      "Requirement already satisfied: tqdm>4.29.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from detectron2==0.6) (4.67.1)\n",
      "Requirement already satisfied: tensorboard in c:\\programdata\\anaconda3\\lib\\site-packages (from detectron2==0.6) (2.20.0)\n",
      "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from detectron2==0.6) (0.1.5.post20221221)\n",
      "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from detectron2==0.6) (0.1.9)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from detectron2==0.6) (2.3.0)\n",
      "Requirement already satisfied: hydra-core>=1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from detectron2==0.6) (1.3.2)\n",
      "Requirement already satisfied: black in c:\\programdata\\anaconda3\\lib\\site-packages (from detectron2==0.6) (24.10.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from detectron2==0.6) (24.2)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2.1.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.2)\n",
      "Requirement already satisfied: portalocker in c:\\programdata\\anaconda3\\lib\\site-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6) (3.2.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\programdata\\anaconda3\\lib\\site-packages (from omegaconf<2.4,>=2.1->detectron2==0.6) (4.9.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>4.29.0->detectron2==0.6) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from black->detectron2==0.6) (8.1.8)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from black->detectron2==0.6) (1.0.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from black->detectron2==0.6) (0.10.3)\n",
      "Requirement already satisfied: platformdirs>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from black->detectron2==0.6) (4.3.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->detectron2==0.6) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->detectron2==0.6) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->detectron2==0.6) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->detectron2==0.6) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->detectron2==0.6) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->detectron2==0.6) (1.17.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\programdata\\anaconda3\\lib\\site-packages (from portalocker->iopath<0.1.10,>=0.1.7->detectron2==0.6) (308)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->detectron2==0.6) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->detectron2==0.6) (1.75.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->detectron2==0.6) (3.8)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->detectron2==0.6) (5.29.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->detectron2==0.6) (72.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard->detectron2==0.6) (3.1.3)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from grpcio>=1.48.2->tensorboard->detectron2==0.6) (4.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git 'C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-req-build-wrv6rz3f'\n"
     ]
    }
   ],
   "source": [
    "# Celula 0: Instalări (rulează o singură dată)\n",
    "!pip install torch torchvision\n",
    "!pip install opencv-python matplotlib numpy\n",
    "!python -m pip install \"git+https://github.com/facebookresearch/detectron2.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe886ec-7e3c-4b47-bfbf-f5da05396f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librăriile au fost importate cu succes.\n"
     ]
    }
   ],
   "source": [
    "# Celula 1: Importuri\n",
    "import torch, detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IMG_DIR = r\"C:\\Users\\Alex\\Desktop\\Proiect Lymphonod\\Proiect Ultrasound\\img\" \n",
    "ANN_DIR = r\"C:\\Users\\Alex\\Desktop\\Proiect Lymphonod\\Proiect Ultrasound\\ann\"  \n",
    "\n",
    "print(\"Librăriile au fost importate cu succes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d07caff1-2acb-448b-a9cc-58f3ebe888c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset înregistrat.\n"
     ]
    }
   ],
   "source": [
    "# Celula 2: Încărcarea Datelor (Suportă BMP)\n",
    "from detectron2.structures import BoxMode\n",
    "import json, os, cv2\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "def get_lymph_node_dicts(img_dir, ann_dir):\n",
    "    dataset_dicts = []\n",
    "    \n",
    "    valid_extensions = ('.jpg', '.png', '.jpeg', '.bmp', '.tif')\n",
    "    files = [f for f in os.listdir(img_dir) if f.lower().endswith(valid_extensions)]\n",
    "    \n",
    "    print(f\"Am găsit {len(files)} fișiere de imagine. Încep procesarea...\")\n",
    "\n",
    "    for idx, filename in enumerate(files):\n",
    "        img_path = os.path.join(img_dir, filename)\n",
    "        \n",
    "        json_file = os.path.join(ann_dir, filename + \".json\")\n",
    "        \n",
    "        if not os.path.exists(json_file):\n",
    "            json_file = os.path.join(ann_dir, os.path.splitext(filename)[0] + \".json\")\n",
    "            \n",
    "        if not os.path.exists(json_file):\n",
    "            # Dacă nici așa nu există, sărim peste\n",
    "            continue \n",
    "\n",
    "        try:\n",
    "            with open(json_file) as f:\n",
    "                imgs_anns = json.load(f)\n",
    "        except:\n",
    "            print(f\"Eroare la citirea JSON: {json_file}\")\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        height, width = img.shape[:2]\n",
    "        \n",
    "        record = {\n",
    "            \"file_name\": img_path,\n",
    "            \"image_id\": filename,\n",
    "            \"height\": height,\n",
    "            \"width\": width,\n",
    "            \"annotations\": []\n",
    "        }\n",
    "      \n",
    "        objects = []\n",
    "        if isinstance(imgs_anns, dict) and 'objects' in imgs_anns:\n",
    "            objects = imgs_anns['objects']\n",
    "        elif isinstance(imgs_anns, list):\n",
    "            objects = imgs_anns\n",
    "\n",
    "        valid_objs = []\n",
    "        for anno in objects:\n",
    "            points = []\n",
    "            if 'points' in anno and 'exterior' in anno['points']:\n",
    "                points = anno['points']['exterior']\n",
    "            elif 'geometry' in anno and 'points' in anno['geometry'] and 'exterior' in anno['geometry']['points']:\n",
    "                points = anno['geometry']['points']['exterior']\n",
    "            \n",
    "            if len(points) > 0:\n",
    "                poly = [float(p) for coord in points for p in coord]\n",
    "                px = poly[0::2]\n",
    "                py = poly[1::2]\n",
    "                \n",
    "                if len(px) >= 3: # Un poligon trebuie să aibă minim 3 puncte\n",
    "                    obj = {\n",
    "                        \"bbox\": [min(px), min(py), max(px), max(py)],\n",
    "                        \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                        \"segmentation\": [poly],\n",
    "                        \"category_id\": 0,\n",
    "                    }\n",
    "                    valid_objs.append(obj)\n",
    "        \n",
    "        if len(valid_objs) > 0:\n",
    "            record[\"annotations\"] = valid_objs\n",
    "            dataset_dicts.append(record)\n",
    "\n",
    "    print(f\" Am încărcat cu succes {len(dataset_dicts)} imagini și adnotări validare.\")\n",
    "    return dataset_dicts\n",
    "\n",
    "DatasetCatalog.clear()\n",
    "MetadataCatalog.clear()\n",
    "\n",
    "DatasetCatalog.register(\"lymph_train\", lambda: get_lymph_node_dicts(IMG_DIR, ANN_DIR))\n",
    "MetadataCatalog.get(\"lymph_train\").set(thing_classes=[\"lymph_node\"])\n",
    "print(\"Dataset înregistrat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "220203f6-9fc6-44aa-81d3-1b3caba7685d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/12 13:53:14 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(28, 28), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(28, 28), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(28, 28), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(28, 28), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[02/12 13:53:14 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip(prob=0.5), RandomRotation(angle=[-45, 45], expand=False), RandomBrightness(intensity_min=0.8, intensity_max=1.2), RandomContrast(intensity_min=0.8, intensity_max=1.2)]\n",
      "Am găsit 52 fișiere de imagine. Încep procesarea...\n",
      "Gata! Am încărcat cu succes 52 imagini și adnotări validare.\n",
      "\u001b[32m[02/12 13:53:15 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 52 images left.\n",
      "\u001b[32m[02/12 13:53:15 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "| lymph_node | 66           |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[02/12 13:53:15 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[02/12 13:53:15 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[02/12 13:53:15 d2.data.common]: \u001b[0mSerializing 52 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[02/12 13:53:15 d2.data.common]: \u001b[0mSerialized dataset takes 0.06 MiB\n",
      "\u001b[32m[02/12 13:53:15 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=2\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[02/12 13:53:15 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
      "\u001b[32m[02/12 13:53:15 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Antrenare Avansată (cu Augmentare și Rezoluție Mare)...\n",
      "Notă: Pe CPU va dura între 45-90 minute. Nu închide fereastra!\n",
      "\u001b[32m[02/12 13:53:16 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4383.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "W0212 13:53:19.932000 18956 site-packages\\torch\\fx\\_symbolic_trace.py:53] is_fx_tracing will return true for both fx.symbolic_trace and torch.export. Please use is_fx_tracing_symbolic_tracing() for specifically fx.symbolic_trace or torch.compiler.is_compiling() for specifically torch.export/compile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/12 13:55:41 d2.engine.hooks]: \u001b[0mOverall training speed: 11 iterations in 0:02:03 (11.2297 s / it)\n",
      "\u001b[32m[02/12 13:55:41 d2.engine.hooks]: \u001b[0mTotal training time: 0:02:03 (0:00:00 on hooks)\n",
      "\u001b[32m[02/12 13:55:41 d2.utils.events]: \u001b[0m eta: 1:40:15  iter: 13  total_loss: 1.939  loss_cls: 0.7634  loss_box_reg: 0.3755  loss_mask: 0.6891  loss_rpn_cls: 0.0382  loss_rpn_loc: 0.00898    time: 10.5006  last_time: 9.9767  data_time: 0.1299  last_data_time: 0.1163   lr: 5.245e-06  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\detectron2\\engine\\train_loop.py:152\u001b[0m, in \u001b[0;36mTrainerBase.train\u001b[1;34m(self, start_iter, max_iter)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbefore_step()\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_step()\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_step()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\detectron2\\engine\\defaults.py:530\u001b[0m, in \u001b[0;36mDefaultTrainer.run_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39miter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter\n\u001b[1;32m--> 530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mrun_step()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\detectron2\\engine\\train_loop.py:319\u001b[0m, in \u001b[0;36mSimpleTrainer.run_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 319\u001b[0m losses\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_backward()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:630\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    622\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    623\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    628\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    629\u001b[0m     )\n\u001b[1;32m--> 630\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    632\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:364\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 364\u001b[0m _engine_run_backward(\n\u001b[0;32m    365\u001b[0m     tensors,\n\u001b[0;32m    366\u001b[0m     grad_tensors_,\n\u001b[0;32m    367\u001b[0m     retain_graph,\n\u001b[0;32m    368\u001b[0m     create_graph,\n\u001b[0;32m    369\u001b[0m     inputs_tuple,\n\u001b[0;32m    370\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    371\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    372\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:865\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    866\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    867\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\tensorboard\\compat\\__init__.py:42\u001b[0m, in \u001b[0;36mtf\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m notf  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tensorboard\\compat\\__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 71\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart Antrenare Avansată (cu Augmentare și Rezoluție Mare)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNotă: Pe CPU va dura între 45-90 minute. Nu închide fereastra!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAntrenare finalizată cu succes!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\detectron2\\engine\\defaults.py:520\u001b[0m, in \u001b[0;36mDefaultTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    514\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;124;03m    Run training.\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \n\u001b[0;32m    517\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;124;03m        OrderedDict of results, if evaluation is enabled. Otherwise None.\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_iter, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter)\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mTEST\u001b[38;5;241m.\u001b[39mEXPECTED_RESULTS) \u001b[38;5;129;01mand\u001b[39;00m comm\u001b[38;5;241m.\u001b[39mis_main_process():\n\u001b[0;32m    522\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[0;32m    523\u001b[0m             \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_last_eval_results\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    524\u001b[0m         ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo evaluation results obtained during training!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\detectron2\\engine\\train_loop.py:162\u001b[0m, in \u001b[0;36mTrainerBase.train\u001b[1;34m(self, start_iter, max_iter)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_train()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\detectron2\\engine\\train_loop.py:171\u001b[0m, in \u001b[0;36mTrainerBase.after_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39miter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hooks:\n\u001b[1;32m--> 171\u001b[0m     h\u001b[38;5;241m.\u001b[39mafter_train()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\detectron2\\engine\\hooks.py:187\u001b[0m, in \u001b[0;36mPeriodicWriter.after_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mafter_train\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m writer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writers:\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;66;03m# If any new data is found (e.g. produced by other after_train),\u001b[39;00m\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;66;03m# write them before closing\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite()\n\u001b[0;32m    188\u001b[0m         writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\detectron2\\utils\\events.py:169\u001b[0m, in \u001b[0;36mTensorboardXWriter.write\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, (v, \u001b[38;5;28miter\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m storage\u001b[38;5;241m.\u001b[39mlatest_with_smoothing_hint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_window_size)\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_write:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writer\u001b[38;5;241m.\u001b[39madd_scalar(k, v, \u001b[38;5;28miter\u001b[39m)\n\u001b[0;32m    170\u001b[0m         new_last_write \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(new_last_write, \u001b[38;5;28miter\u001b[39m)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_write \u001b[38;5;241m=\u001b[39m new_last_write\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\functools.py:1026\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m   1024\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m-> 1026\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(instance)\n\u001b[0;32m   1027\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1028\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\detectron2\\utils\\events.py:160\u001b[0m, in \u001b[0;36mTensorboardXWriter._writer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_writer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SummaryWriter(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writer_args)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m Version\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m tensorboard\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FileWriter, SummaryWriter\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecord_writer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecordWriter\n\u001b[0;32m     15\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFileWriter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecordWriter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummaryWriter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m ]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevent_file_writer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EventFileWriter\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_convert_np\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_np\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_embedding\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_embedding_info, make_mat, make_sprite, make_tsv, write_pbtxt\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_onnx_graph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_onnx_graph\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytorch_graph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m graph\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\tensorboard\\_embedding.py:10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprojector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprojector_config_pb2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EmbeddingInfo\n\u001b[1;32m---> 10\u001b[0m _HAS_GFILE_JOIN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_gfile_join\u001b[39m(a, b):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# The join API is different between tensorboard's TF stub and TF:\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# https://github.com/tensorflow/tensorboard/issues/6080\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# We need to try both because `tf` may point to either the stub or the real TF.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _HAS_GFILE_JOIN:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\tensorboard\\lazy.py:65\u001b[0m, in \u001b[0;36mlazy_load.<locals>.wrapper.<locals>.LazyModule.__getattr__\u001b[1;34m(self, attr_name)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name):\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(load_once(\u001b[38;5;28mself\u001b[39m), attr_name)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\tensorboard\\lazy.py:97\u001b[0m, in \u001b[0;36m_memoize.<locals>.wrapper\u001b[1;34m(arg)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m cache\u001b[38;5;241m.\u001b[39mget(arg, nothing) \u001b[38;5;129;01mis\u001b[39;00m nothing:\n\u001b[1;32m---> 97\u001b[0m             cache[arg] \u001b[38;5;241m=\u001b[39m f(arg)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cache[arg]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\tensorboard\\lazy.py:50\u001b[0m, in \u001b[0;36mlazy_load.<locals>.wrapper.<locals>.load_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m load_once\u001b[38;5;241m.\u001b[39mloading \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m     module \u001b[38;5;241m=\u001b[39m load_fn()\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     load_once\u001b[38;5;241m.\u001b[39mloading \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\tensorboard\\compat\\__init__.py:45\u001b[0m, in \u001b[0;36mtf\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 45\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tensorflow\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-import-not-at-top,line-too-long,undefined-variable\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Celula 3 : Antrenare cu Augmentare \n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.data import build_detection_train_loader\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.data import DatasetMapper\n",
    "import os\n",
    "\n",
    "class LymphTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        augs = [\n",
    "            T.ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'),\n",
    "            \n",
    "            T.RandomFlip(prob=0.5, horizontal=True, vertical=False),\n",
    "            \n",
    "            T.RandomRotation(angle=[-45, 45], expand=False),\n",
    "            \n",
    "            T.RandomBrightness(0.8, 1.2),\n",
    "            T.RandomContrast(0.8, 1.2)\n",
    "        ]\n",
    "        \n",
    "        mapper = DatasetMapper(cfg, is_train=True, augmentations=augs)\n",
    "        return build_detection_train_loader(cfg, mapper=mapper)\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"lymph_train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 0 \n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "\n",
    "cfg.MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION = 28 \n",
    "\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "\n",
    "cfg.SOLVER.MAX_ITER = 600 \n",
    "\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "\n",
    "cfg.MODEL.DEVICE = \"cpu\"\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "trainer = LymphTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "\n",
    "print(\"Start Antrenare Avansată (cu Augmentare și Rezoluție Mare)...\")\n",
    "print(\"Notă: Pe CPU va dura între 45-90 minute. Nu închide fereastra!\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Antrenare finalizată cu succes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dc23e70-57bd-4c6d-b73b-db9fe2f22bee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_lymph_node_dicts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBENIGN (Normal)\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: dataset_dicts\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m: dataset_dicts \u001b[38;5;241m=\u001b[39m get_lymph_node_dicts(IMG_DIR, ANN_DIR)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_dicts' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBENIGN (Normal)\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: dataset_dicts\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m: dataset_dicts \u001b[38;5;241m=\u001b[39m get_lymph_node_dicts(IMG_DIR, ANN_DIR)\n\u001b[0;32m     63\u001b[0m d \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(dataset_dicts)\n\u001b[0;32m     64\u001b[0m im_original \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_lymph_node_dicts' is not defined"
     ]
    }
   ],
   "source": [
    "# Celula 4 (ULTRA-RAFINATĂ): Contrast Boost + Tight Fit (Eroziune)\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.65 \n",
    "cfg.MODEL.DEVICE = \"cpu\"\n",
    "cfg.MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION = 28\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "\n",
    "def enhance_contrast(image):\n",
    "    \"\"\"\n",
    "    Mărește contrastul local (CLAHE) pentru ca marginile să fie BISTURIU.\n",
    "    \"\"\"\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(l)\n",
    "    limg = cv2.merge((cl,a,b))\n",
    "    final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "    return final\n",
    "\n",
    "def refine_mask_tight(mask_raw):\n",
    "    \"\"\"\n",
    "    Combină închiderea găurilor cu EROZIUNEA (micșorarea) pentru a strânge conturul.\n",
    "    \"\"\"\n",
    "    kernel_close = np.ones((5,5), np.uint8)\n",
    "    mask_closed = cv2.morphologyEx(mask_raw, cv2.MORPH_CLOSE, kernel_close)\n",
    "    \n",
    "    kernel_erode = np.ones((3,3), np.uint8)\n",
    "    mask_tight = cv2.erode(mask_closed, kernel_erode, iterations=2)\n",
    "    \n",
    "    contours, _ = cv2.findContours(mask_tight, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours: return mask_raw, None\n",
    "    \n",
    "    cnt = contours[0]\n",
    "    epsilon = 0.003 * cv2.arcLength(cnt, True) \n",
    "    approx_curve = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "    \n",
    "    return mask_tight, approx_curve\n",
    "\n",
    "def get_diagnosis(shape_ratio, short_axis_mm):\n",
    "    score = 0\n",
    "    if shape_ratio > 0.6: score += 1\n",
    "    if short_axis_mm > 9.0: score += 1\n",
    "    \n",
    "    if score >= 1: return \"MALIGN (Suspect)\", (255, 0, 0)\n",
    "    else: return \"BENIGN (Normal)\", (0, 255, 0)\n",
    "\n",
    "try: dataset_dicts\n",
    "except NameError: dataset_dicts = get_lymph_node_dicts(IMG_DIR, ANN_DIR)\n",
    "\n",
    "d = random.choice(dataset_dicts)\n",
    "im_original = cv2.imread(d[\"file_name\"])\n",
    "\n",
    "im_enhanced = enhance_contrast(im_original)\n",
    "\n",
    "print(f\"Analizez imaginea (High Contrast): {os.path.basename(d['file_name'])}\")\n",
    "\n",
    "outputs = predictor(im_enhanced)\n",
    "instances = outputs[\"instances\"].to(\"cpu\")\n",
    "\n",
    "img_display = im_original.copy() \n",
    "\n",
    "if len(instances) > 0:\n",
    "    best = instances[0]\n",
    "    score_ai = best.scores.numpy()[0]\n",
    "    mask_raw = best.pred_masks[0].numpy().astype('uint8') * 255\n",
    "    \n",
    "    mask_final, tight_contour = refine_mask_tight(mask_raw)\n",
    "    \n",
    "    if tight_contour is not None:\n",
    "        if len(tight_contour) >= 5:\n",
    "            ellipse = cv2.fitEllipse(tight_contour)\n",
    "            MA, ma = ellipse[1]\n",
    "            PIXEL_TO_MM = 0.108 \n",
    "            \n",
    "            long_axis = max(MA, ma) * PIXEL_TO_MM\n",
    "            short_axis = min(MA, ma) * PIXEL_TO_MM\n",
    "            area = cv2.contourArea(tight_contour) * (PIXEL_TO_MM ** 2)\n",
    "            ratio = short_axis / long_axis\n",
    "            \n",
    "            gray = cv2.cvtColor(im_original, cv2.COLOR_BGR2GRAY)\n",
    "            mask_edge = cv2.Canny(mask_final, 100, 200)\n",
    "            grad_val = cv2.mean(gray, mask=mask_edge)[0]\n",
    "            \n",
    "            diag_text, color_bgr = get_diagnosis(ratio, short_axis)\n",
    "            color_draw = (color_bgr[2], color_bgr[1], color_bgr[0])\n",
    "            \n",
    "            cv2.drawContours(img_display, [tight_contour], -1, color_draw, 2)\n",
    "            \n",
    "            infos = [\n",
    "                f\"S/L Ratio:  {ratio:.2f}\",\n",
    "                f\"Short Axis: {short_axis:.1f} mm\",\n",
    "                f\"Area:       {area:.0f} mm2\",\n",
    "                f\"Sharpness:  {grad_val:.1f}\",\n",
    "                f\"AI Conf:    {score_ai*100:.0f}%\",\n",
    "                f\"RESULT:     {diag_text}\"\n",
    "            ]\n",
    "            \n",
    "            y0, dy = 30, 25\n",
    "            print(\"\\n=== REZULTATE RAFINATE ===\")\n",
    "            for i, line in enumerate(infos):\n",
    "                print(f\"* {line}\")\n",
    "                y = y0 + i * dy\n",
    "                cv2.putText(img_display, line, (10, y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 4)\n",
    "                cv2.putText(img_display, line, (10, y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color_draw, 2)\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(im_enhanced[:, :, ::-1])\n",
    "    plt.title(\"Ce vede AI-ul (Contrast Boosted)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img_display[:, :, ::-1])\n",
    "    plt.title(f\"Rezultat Final (Tight Fit)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Nicio detecție. Imaginea e prea dificilă sau contrastul e prea mic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897c231-23e8-4d0a-a790-2524e4933fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
